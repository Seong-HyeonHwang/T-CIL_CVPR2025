{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device is cuda:7\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:7' if is_cuda else 'cpu')\n",
    "print('Current cuda device is', device)\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n",
    "                                     std=[0.2675, 0.2565, 0.2761])\n",
    "transform_train = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "number of training data :  50000\n",
      "number of test data :  10000\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR100(root = '../CIFAR-100/data/02/',\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transform_train)\n",
    "test_data = datasets.CIFAR100(root = '../CIFAR-100/data/02/',\n",
    "                            train=False,\n",
    "                            download=True,\n",
    "                            transform=transform_test)\n",
    "print('number of training data : ', len(train_data))\n",
    "print('number of test data : ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "num_task = 10\n",
    "num_class = 100\n",
    "per_num_class = num_class//num_task\n",
    "train_task = {x: [] for x in range(num_task)}\n",
    "test_task = {x: [] for x in range(num_task)}\n",
    "\n",
    "train_class_idx = {x: [] for x in range(num_class)}\n",
    "test_class_idx = {x: [] for x in range(num_class)}\n",
    "\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    x, y = data\n",
    "    train_class_idx[y].append(cnt)\n",
    "    cnt +=1\n",
    "    \n",
    "print(len(train_class_idx[0]))\n",
    "\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    x, y = data\n",
    "    test_class_idx[y].append(cnt)\n",
    "    cnt +=1\n",
    "    \n",
    "for i in range(num_task):\n",
    "    curr_task_idx_train = []\n",
    "    curr_task_idx_test = []\n",
    "    for j in range(per_num_class):\n",
    "        curr_task_idx_train += train_class_idx[i*per_num_class+j]\n",
    "        curr_task_idx_test += test_class_idx[i*per_num_class+j]\n",
    "    train_task[i] = [train_data[j] for j in curr_task_idx_train]\n",
    "    test_task[i] = [test_data[j] for j in curr_task_idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-CIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import evaluation, tune_temp, tune_temp_batch, adaptive_evaluation, tune_temp_batch_efficient\n",
    "from utils import cal_ece, cal_aece, tune_temp_batch_efficient, set_seed\n",
    "from TCIL import AdversarialTrainer, find_optimal_epsilon\n",
    "from model import resnet32\n",
    "method_type = 'ER'\n",
    "num_tasks = 10\n",
    "num_classes = 100\n",
    "num_class_per_task = num_classes//num_tasks\n",
    "batch_size = 128\n",
    "# seed_list = [1, 2, 3, 4, 5]\n",
    "seed_list = [1]\n",
    "val_size = 100\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running experiment with seed 1 ===\n",
      "[Task 0] ece_overall: 3.01, aece_overall: 2.64\n",
      "[Task 1] ece_overall: 11.11, aece_overall: 11.10\n",
      "[Task 2] ece_overall: 5.66, aece_overall: 5.88\n",
      "[Task 3] ece_overall: 4.49, aece_overall: 4.67\n",
      "[Task 4] ece_overall: 3.43, aece_overall: 3.48\n",
      "[Task 5] ece_overall: 4.32, aece_overall: 4.34\n",
      "[Task 6] ece_overall: 5.90, aece_overall: 5.73\n",
      "[Task 7] ece_overall: 3.92, aece_overall: 4.08\n",
      "[Task 8] ece_overall: 6.19, aece_overall: 6.19\n",
      "[Task 9] ece_overall: 9.35, aece_overall: 9.35\n",
      "[Seed 1 Avg] ece_overall: 5.74, aece_overall: 5.75\n",
      "\n",
      "=== Final Results ===\n",
      "ECE Overall - Mean: 5.74%, Std: 0.00%, AECE Overall - Mean: 5.75%, Std: 0.00%\n"
     ]
    }
   ],
   "source": [
    "all_seeds_ece_overall = []\n",
    "all_seeds_ece_task = []\n",
    "all_seeds_aece_overall = []\n",
    "all_seeds_aece_task = []\n",
    "\n",
    "for seed in seed_list:\n",
    "    print(f\"\\n=== Running experiment with seed {seed} ===\")\n",
    "    ece_overall_hist = []\n",
    "    ece_task_hist = []\n",
    "    aece_overall_hist = []\n",
    "    aece_task_hist = []\n",
    "    set_seed(seed)\n",
    "    for t in range(num_tasks):\n",
    "        test_task_total = copy.deepcopy(test_task[0])\n",
    "        if t > 0:\n",
    "            for i in range(1, t+1):\n",
    "                test_task_total += test_task[i]\n",
    "        model = resnet32().to(device)\n",
    "        model.load_state_dict(torch.load(f'./saved_model/{method_type}_{seed}_seed_{num_task}_tasks_{val_size}_val_{t}_task.pt', weights_only=True))\n",
    "        model.eval()\n",
    "        buffer_indices = np.load(f'./saved_buffer_indices/{method_type}_buffer_indices_{seed}_seed_{num_task}_tasks_{val_size}_val_{t}_task.npy')\n",
    "        buffer = torch.utils.data.Subset(train_data, buffer_indices)\n",
    "        valid_new_task_indices = np.load(f'./saved_buffer_indices/{method_type}_valid_indices_{seed}_seed_{num_task}_tasks_{val_size}_val_{t}_task.npy')\n",
    "        valid_new_task = torch.utils.data.Subset(train_data, valid_new_task_indices)\n",
    "        temperature_new_task_opt = tune_temp_batch_efficient(model, valid_new_task, (t+1)*num_class_per_task, epochs, batch_size, device).item()\n",
    "        \n",
    "        new_task_idx = []\n",
    "        for j, (data, label) in enumerate(buffer):\n",
    "            if label // num_class_per_task == t:\n",
    "                new_task_idx.append(j)\n",
    "        new_task_data = torch.utils.data.Subset(buffer, new_task_idx)\n",
    "        \n",
    "        trainer = AdversarialTrainer(model, device, method_type)\n",
    "        \n",
    "        best_epsilon = find_optimal_epsilon(\n",
    "            trainer=trainer,\n",
    "            buffer_data=buffer,\n",
    "            valid_data=new_task_data,\n",
    "            target_temp=temperature_new_task_opt,\n",
    "            num_class=(t+1)*num_class_per_task,\n",
    "            num_task=t+1,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        adv_data, labels = trainer.generate_adversarial_data(buffer, buffer, \n",
    "                                                               (t+1)*num_class_per_task, batch_size, best_epsilon, num_class_per_task)\n",
    "        adv_dataset = torch.utils.data.TensorDataset(adv_data, labels)\n",
    "        adv_loader = torch.utils.data.DataLoader(adv_dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "        temperature = trainer.get_temperature(adv_loader, (t+1)*num_class_per_task, (t+1), epochs, batch_size).item()\n",
    "        \n",
    "        ece_overall, Bm, acc, conf, _, _ = cal_ece(model, test_task_total, (t+1)*num_class_per_task, num_class_per_task,\n",
    "                                              batch_size, n_bins = 10, temperature = temperature, device = device)\n",
    "        aece_overall, _, _, _ = cal_aece(model, test_task_total, (t+1)*num_class_per_task, num_class_per_task,\n",
    "                                                batch_size, n_bins = 10, temperature = temperature, device = device)\n",
    "        ece_overall_hist.append(ece_overall)\n",
    "        aece_overall_hist.append(aece_overall)\n",
    "        print(f\"[Task {t}] ece_overall: {ece_overall*100:.2f}, aece_overall: {aece_overall*100:.2f}\")\n",
    "\n",
    "    \n",
    "    print(f\"[Seed {seed} Avg] ece_overall: {np.mean(ece_overall_hist)*100:.2f}, aece_overall: {np.mean(aece_overall_hist)*100:.2f}\")\n",
    "    all_seeds_ece_overall.append(np.mean(ece_overall_hist)*100)\n",
    "    all_seeds_aece_overall.append(np.mean(aece_overall_hist)*100)\n",
    "\n",
    "final_ece_overall_mean = np.mean(all_seeds_ece_overall)\n",
    "final_ece_overall_std = np.std(all_seeds_ece_overall)\n",
    "\n",
    "final_aece_overall_mean = np.mean(all_seeds_aece_overall)\n",
    "final_aece_overall_std = np.std(all_seeds_aece_overall)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"ECE Overall - Mean: {final_ece_overall_mean:.2f}%, Std: {final_ece_overall_std:.2f}%, AECE Overall - Mean: {final_aece_overall_mean:.2f}%, Std: {final_aece_overall_std:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
