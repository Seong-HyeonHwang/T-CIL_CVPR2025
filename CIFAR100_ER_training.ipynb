{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device is cuda:7\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:7' if is_cuda else 'cpu')\n",
    "\n",
    "print('Current cuda device is', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n",
    "                                     std=[0.2675, 0.2565, 0.2761])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data :  50000\n",
      "number of test data :  10000\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR100(root = '../CIFAR-100/data/02/',\n",
    "                            train=True,\n",
    "                            download=False,\n",
    "                            transform=transform_train)\n",
    "test_data = datasets.CIFAR100(root = '../CIFAR-100/data/02/',\n",
    "                            train=False,\n",
    "                            download=False,\n",
    "                            transform=transform_test)\n",
    "print('number of training data : ', len(train_data))\n",
    "print('number of test data : ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "num_task = 10\n",
    "num_class = 100\n",
    "num_class_per_task = num_class//num_task\n",
    "train_task = {x: [] for x in range(num_task)}\n",
    "test_task = {x: [] for x in range(num_task)}\n",
    "\n",
    "train_class_idx = {x: [] for x in range(num_class)}\n",
    "test_class_idx = {x: [] for x in range(num_class)}\n",
    "\n",
    "cnt = 0\n",
    "for data in train_data:\n",
    "    x, y = data\n",
    "    train_class_idx[y].append(cnt)\n",
    "    cnt +=1\n",
    "    \n",
    "print(len(train_class_idx[0]))\n",
    "\n",
    "cnt = 0\n",
    "for data in test_data:\n",
    "    x, y = data\n",
    "    test_class_idx[y].append(cnt)\n",
    "    cnt +=1\n",
    "    \n",
    "for i in range(num_task):\n",
    "    curr_task_idx_train = []\n",
    "    curr_task_idx_test = []\n",
    "    for j in range(num_class_per_task):\n",
    "        curr_task_idx_train += train_class_idx[i*num_class_per_task+j]\n",
    "        curr_task_idx_test += test_class_idx[i*num_class_per_task+j]\n",
    "    train_task[i] = [train_data[j] for j in curr_task_idx_train]\n",
    "    test_task[i] = [test_data[j] for j in curr_task_idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_task[0]))\n",
    "print(len(train_task[4]))\n",
    "print(len(train_class_idx[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 1e-1\n",
    "num_epochs = 200\n",
    "val_size = 100\n",
    "total_memory_size = 2000\n",
    "method_type = 'ER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-19 21:40:06.563346\n",
      "---task 0---\n",
      "train_size: 4900, val_size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 200/200 [02:36<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "[Acc]\n",
      "task 0: 87.80\n",
      "---task 1---\n",
      "train_size: 4900, val_size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 200/200 [03:12<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "[Acc]\n",
      "task 0: 73.90\n",
      "task 1: 82.00\n",
      "---task 2---\n",
      "train_size: 4900, val_size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 200/200 [03:11<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n",
      "[Acc]\n",
      "task 0: 61.30\n",
      "task 1: 59.80\n",
      "task 2: 87.10\n",
      "---task 3---\n",
      "train_size: 4900, val_size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                    | 2/200 [00:02<03:30,  1.06s/it]"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from model import resnet32\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(datetime.now())\n",
    "acc_list=[]\n",
    "forget_list = []\n",
    "task_order = np.arange(num_task)\n",
    "milestones=[100, 150]\n",
    "start_time = time.time()\n",
    "PATH_model = './saved_model/'\n",
    "PATH_buffer = './saved_buffer_indices/'\n",
    "\n",
    "for seed in [1]:\n",
    "    set_seed(seed)\n",
    "    model = resnet32().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    final_avg_acc = 0\n",
    "\n",
    "    for t in task_order:\n",
    "        print(f\"---task {t:d}---\")\n",
    "        optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9, weight_decay=2e-4)\n",
    "        scheduler = MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=0.1)\n",
    "        num_total_class = (t+1) * num_class_per_task\n",
    "        num_known_class = t * num_class_per_task\n",
    "        \n",
    "        new_task_data_idx = []\n",
    "        for i in range(num_class_per_task):\n",
    "            new_task_data_idx += train_class_idx[num_known_class + i]\n",
    "        num_data_per_class_new = len(new_task_data_idx)//num_class_per_task\n",
    "\n",
    "        num_data_per_class_valid = val_size // num_class_per_task\n",
    "        valid_idx = []\n",
    "        for i in range(num_class_per_task):\n",
    "            start_idx = i*num_data_per_class_new\n",
    "            end_idx = (i+1)*num_data_per_class_new\n",
    "            valid_idx += random.sample(new_task_data_idx[start_idx:end_idx], num_data_per_class_valid)\n",
    "        new_task_data_idx = [x for x in new_task_data_idx if x not in valid_idx]\n",
    "        num_data_per_class_new = len(new_task_data_idx)//num_class_per_task\n",
    "        new_task_data = torch.utils.data.Subset(train_data, new_task_data_idx)\n",
    "        print(f\"train_size: {len(new_task_data)}, val_size: {len(valid_idx)}\")\n",
    "        \n",
    "        if t == 0:\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=new_task_data,\n",
    "                                                       batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "        else:\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=new_task_data + buffer,\n",
    "                                                        batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "            \n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output[:, :num_total_class], target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        torch.save(model.state_dict(), PATH_model+f'{method_type}_{seed}_seed_{num_task}_tasks_{val_size}_val_{t}_task.pt')\n",
    "        \n",
    "        if t > 0:\n",
    "            num_data_per_class_buffer_prev = total_memory_size // (num_known_class)\n",
    "        num_data_per_class_buffer = total_memory_size // (num_total_class)\n",
    "        temp_buffer_idx = []\n",
    "        for i in range(num_total_class):\n",
    "            if i < num_known_class:\n",
    "                start_idx = i*num_data_per_class_buffer_prev\n",
    "                end_idx = (i+1)*num_data_per_class_buffer_prev\n",
    "                temp_buffer_idx += random.sample(buffer_idx[start_idx:end_idx], num_data_per_class_buffer)\n",
    "            else:\n",
    "                temp_idx = i - num_known_class\n",
    "                start_idx = temp_idx*num_data_per_class_new\n",
    "                end_idx = (temp_idx+1)*num_data_per_class_new\n",
    "                temp_buffer_idx += random.sample(new_task_data_idx[start_idx:end_idx], num_data_per_class_buffer)\n",
    "        buffer_idx = copy.deepcopy(temp_buffer_idx)\n",
    "        \n",
    "        buffer = torch.utils.data.Subset(train_data, buffer_idx)\n",
    "        print(len(buffer))\n",
    "        \n",
    "        np.save(PATH_buffer + f'{method_type}_buffer_indices_{seed}_seed_{num_task}_tasks_{val_size}_val_{t}_task', np.array(buffer_idx))\n",
    "        np.save(PATH_buffer + f'{method_type}_valid_indices_{seed}_seed_{num_task}_tasks_{val_size}_val_{t}_task', np.array(valid_idx))\n",
    "        np.save(PATH_buffer + f'{method_type}_train_indices_{seed}_seed_{num_task}_tasks_{val_size}_val_{t}_task', np.array(new_task_data_idx))\n",
    "        print(f\"[Acc]\")\n",
    "        model.eval()\n",
    "        avg_acc = 0\n",
    "        for test_task_idx in range(t+1):\n",
    "            correct = 0\n",
    "            test_loader = torch.utils.data.DataLoader(dataset=test_task[test_task_idx],\n",
    "                                                       batch_size = batch_size, shuffle = False)\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)[:, :num_total_class]\n",
    "                    prediction = output.data.max(1)[1]\n",
    "                    correct += prediction.eq(target.data).sum()\n",
    "                temp_acc = correct/len(test_loader.dataset)\n",
    "            avg_acc += temp_acc / (t+1)\n",
    "            print(f\"task {test_task_idx:d}: {temp_acc*100:.2f}\")\n",
    "        final_avg_acc += avg_acc/num_task\n",
    "    print(f\"SEED: {seed}, [Total] Acc: {final_avg_acc*100:.2f}\\n\")\n",
    "print(f\"runtime: {(time.time()-start_time)/60:.2f} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Val size: 0\n",
    "SEED: 1, [Total] Acc: 56.75\n",
    "SEED: 2, [Total] Acc: 56.38\n",
    "SEED: 3, [Total] Acc: 56.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch\n",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
